{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fafc096",
   "metadata": {
    "papermill": {
     "duration": 0.004585,
     "end_time": "2023-03-13T15:36:35.840839",
     "exception": false,
     "start_time": "2023-03-13T15:36:35.836254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"padding:10px; \n",
    "            color:#FF9F00;\n",
    "            margin:10px;\n",
    "            font-size:150%;\n",
    "            display:fill;\n",
    "            border-radius:1px;\n",
    "            border-style: solid;\n",
    "            border-color:#FF9F00;\n",
    "            background-color:#3E3D53;\n",
    "            overflow:hidden;\">\n",
    "    <center>\n",
    "        <a id='top'></a>\n",
    "        <b>Table of Contents</b>\n",
    "    </center>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"#1\">1 -  Overview and Imports</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"#2\">2 - Data Preparation</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"#3\">3 - GRU Implementation</a>\n",
    "        <li>\n",
    "            <a href=\"#4\">4 - Thank you</a>\n",
    "        </li> \n",
    "        <li>\n",
    "            <a href=\"#5\">5 - References</a>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<a id=\"1\"></a>\n",
    "\n",
    "<h1 style='background:#FF9F00;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '><center style='color: #3E3D53;'>Overview and Imports</center></h1>\n",
    "    \n",
    "# Overview and Imports\n",
    "\n",
    "**Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) that are designed to handle sequential data, such as time series or natural language text. They achieve this by using a memory cell that can selectively remember or forget information at each time step of the input sequence, allowing the network to maintain a memory of previous inputs over a longer period of time.**\n",
    "\n",
    "**This notebook contains an implementation of an LSTM that can be used for language modeling. The self takes in a sequence of characters and outputs the probability distribution over the next character in the sequence. The network is trained on a corpus of text and then used to generate new text that has a similar distribution of characters as the training corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec0de0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T15:36:35.850372Z",
     "iopub.status.busy": "2023-03-13T15:36:35.849901Z",
     "iopub.status.idle": "2023-03-13T15:36:35.884482Z",
     "shell.execute_reply": "2023-03-13T15:36:35.883313Z"
    },
    "papermill": {
     "duration": 0.04283,
     "end_time": "2023-03-13T15:36:35.887419",
     "exception": false,
     "start_time": "2023-03-13T15:36:35.844589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5fbe9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T15:36:35.896771Z",
     "iopub.status.busy": "2023-03-13T15:36:35.896372Z",
     "iopub.status.idle": "2023-03-13T15:36:35.909285Z",
     "shell.execute_reply": "2023-03-13T15:36:35.907773Z"
    },
    "papermill": {
     "duration": 0.020783,
     "end_time": "2023-03-13T15:36:35.912077",
     "exception": false,
     "start_time": "2023-03-13T15:36:35.891294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for reading and preprocessing text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, sequence_length: int):\n",
    "        \"\"\"\n",
    "        Initializes a DataReader object with the path to a text file and the desired sequence length.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file.\n",
    "            sequence_length (int): The length of the sequences that will be fed to the self.\n",
    "        \"\"\"\n",
    "        with open(path) as f:\n",
    "            # Read the contents of the file\n",
    "            self.data = f.read()\n",
    "\n",
    "        # Find all unique characters in the text\n",
    "        chars = list(set(self.data))\n",
    "\n",
    "        # Create dictionaries to map characters to indices and vice versa\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(chars)}\n",
    "\n",
    "        # Store the size of the text data and the size of the vocabulary\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        # Initialize the pointer that will be used to generate sequences\n",
    "        self.pointer = 0\n",
    "\n",
    "        # Store the desired sequence length\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Generates a batch of input and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            inputs_one_hot (np.ndarray): A numpy array with shape `(batch_size, vocab_size)` where each row is a one-hot encoded representation of a character in the input sequence.\n",
    "            targets (list): A list of integers that correspond to the indices of the characters in the target sequence, which is the same as the input sequence shifted by one position to the right.\n",
    "        \"\"\"\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.sequence_length\n",
    "\n",
    "        # Get the input sequence as a list of integers\n",
    "        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n",
    "\n",
    "        # One-hot encode the input sequence\n",
    "        inputs_one_hot = np.zeros((len(inputs), self.vocab_size))\n",
    "        inputs_one_hot[np.arange(len(inputs)), inputs] = 1\n",
    "\n",
    "        # Get the target sequence as a list of integers\n",
    "        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n",
    "\n",
    "        # Update the pointer\n",
    "        self.pointer += self.sequence_length\n",
    "\n",
    "        # Reset the pointer if the next batch would exceed the length of the text data\n",
    "        if self.pointer + self.sequence_length + 1 >= self.data_size:\n",
    "            self.pointer = 0\n",
    "\n",
    "        return inputs_one_hot, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440d46cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T15:36:35.922644Z",
     "iopub.status.busy": "2023-03-13T15:36:35.921881Z",
     "iopub.status.idle": "2023-03-13T15:36:35.993563Z",
     "shell.execute_reply": "2023-03-13T15:36:35.992576Z"
    },
    "papermill": {
     "duration": 0.080608,
     "end_time": "2023-03-13T15:36:35.996271",
     "exception": false,
     "start_time": "2023-03-13T15:36:35.915663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (GRU).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the GR.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the GRU.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the GRU.\n",
    "    self.learning_rate : float\n",
    "        The learning rate used during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
    "        Initializes an instance of the GRU class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the GRU class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the GRU.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the GRU.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the GRU.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # model parameters\n",
    "        self.Wz = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wr = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (vocab_size, hidden_size))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "\n",
    "        # initialize gradients for each parameter\n",
    "        self.dWz, self.dWr, self.dWa, self.dWy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(\n",
    "            self.Wa), np.zeros_like(self.Wy)\n",
    "        self.dbz, self.dbr, self.dba, self.dby = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(\n",
    "            self.bz), np.zeros_like(self.by)\n",
    "\n",
    "        # initialize parameters for adamw optimizer\n",
    "        self.mWz = np.zeros_like(self.Wz)\n",
    "        self.vWz = np.zeros_like(self.Wz)\n",
    "        self.mWr = np.zeros_like(self.Wr)\n",
    "        self.vWr = np.zeros_like(self.Wr)\n",
    "        self.mWa = np.zeros_like(self.Wa)\n",
    "        self.vWa = np.zeros_like(self.Wa)\n",
    "        self.mWy = np.zeros_like(self.Wy)\n",
    "        self.vWy = np.zeros_like(self.Wy)\n",
    "        self.mbz = np.zeros_like(self.bz)\n",
    "        self.vbz = np.zeros_like(self.bz)\n",
    "        self.mbr = np.zeros_like(self.br)\n",
    "        self.vbr = np.zeros_like(self.br)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, c_prev, a_prev):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a simple GRU model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "\n",
    "        Returns: X (numpy array): Input sequence, shape (sequence_length, input_size) c (dictionary): Cell state for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) r (dictionary): Reset gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) z (dictionary): Update gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) cc (dictionary): Candidate cell\n",
    "        state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) a (dictionary):\n",
    "        Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) y_pred (\n",
    "        dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (\n",
    "        output_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize dictionaries for backpropagation\n",
    "        # initialize dictionaries for backpropagation\n",
    "        r, z, c, cc, a, y_pred = {}, {}, {}, {}, {}, {}\n",
    "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
    "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
    "\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(X.shape[0]):\n",
    "            # concatenate the input and hidden state\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a[t - 1], xt))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r[t] = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z[t] = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc[t] = np.tanh(np.dot(self.Wa, np.vstack((r[t] * a[t - 1], xt))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c[t] = z[t] * cc[t] + (1 - z[t]) * c[t - 1]\n",
    "\n",
    "            # compute the hidden state\n",
    "            a[t] = c[t]\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
    "\n",
    "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
    "        return X, r, z, c, cc, a, y_pred\n",
    "\n",
    "    def backward(self, X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through time for a GRU network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "            r (dictionary): Reset gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            z (dictionary): Update gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
    "            targets (numpy array): Target outputs for each time step, shape (sequence_length, output_size)\n",
    "\n",
    "        Returns:\n",
    "            None       \n",
    "        \"\"\"\n",
    "        # Initialize gradients for hidden state\n",
    "        dc_next = np.zeros_like(c_prev)\n",
    "        da_next = np.zeros_like(a_prev)\n",
    "\n",
    "        # Iterate backwards through time steps\n",
    "        for t in reversed(range(X.shape[0])):\n",
    "            # compute the gradient of the output probability vector\n",
    "            dy = np.copy(y_pred[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # compute the gradient of the output layer weights and biases\n",
    "            self.dWy += np.dot(dy, a[t].T)\n",
    "            self.dby += dy\n",
    "\n",
    "            # compute the gradient of the hidden state\n",
    "            da = np.dot(self.Wy.T, dy) + da_next\n",
    "\n",
    "            # compute the gradient of the update gate\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a_prev, xt))\n",
    "            dz = da * (a[t] - c[t])\n",
    "            self.dWz += np.dot(dz, concat.T)\n",
    "            self.dbz += dz\n",
    "\n",
    "            # compute the gradient of the reset gate\n",
    "            dr = da * np.dot(self.Wz[:, :self.hidden_size].T, dz) * (1 - r[t]) * r[t]\n",
    "            self.dWr += np.dot(dr, concat.T)\n",
    "            self.dbr += dr\n",
    "\n",
    "            # compute the gradient of the current hidden state\n",
    "            da = np.dot(self.Wa[:, :self.hidden_size].T, dr) + np.dot(self.Wz[:, :self.hidden_size].T, dz)\n",
    "            self.dWa += np.dot(da * (1 - a[t]**2), concat.T)\n",
    "            self.dba += da * (1 - a[t]**2)\n",
    "\n",
    "            # compute the gradient of the input to the next hidden state\n",
    "            da_next = np.dot(self.Wr[:, :self.hidden_size].T, dr) \\\n",
    "                      + np.dot(self.Wz[:, :self.hidden_size].T, dz) \\\n",
    "                      + np.dot(self.Wa[:, :self.hidden_size].T, da)\n",
    "        # clip gradients to avoid exploding gradients\n",
    "        for grad in [self.dWz, self.dWr, self.dWa, self.dWy, self.dbz, self.dbr, self.dba, self.dby]:\n",
    "            np.clip(grad, -1, 1)\n",
    "\n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
    "\n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the GRU's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # AdamW update for Wz\n",
    "        self.mWz = beta1 * self.mWz + (1 - beta1) * self.dWz\n",
    "        self.vWz = beta2 * self.vWz + (1 - beta2) * np.square(self.dWz)\n",
    "        m_hat = self.mWz / (1 - beta1)\n",
    "        v_hat = self.vWz / (1 - beta2)\n",
    "        self.Wz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wz)\n",
    "\n",
    "        # AdamW update for bu\n",
    "        self.mbz = beta1 * self.mbz + (1 - beta1) * self.dbz\n",
    "        self.vbz = beta2 * self.vbz + (1 - beta2) * np.square(self.dbz)\n",
    "        m_hat = self.mbz / (1 - beta1)\n",
    "        v_hat = self.vbz / (1 - beta2)\n",
    "        self.bz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bz)\n",
    "\n",
    "        # AdamW update for Wr\n",
    "        self.mWr = beta1 * self.mWr + (1 - beta1) * self.dWr\n",
    "        self.vWr = beta2 * self.vWr + (1 - beta2) * np.square(self.dWr)\n",
    "        m_hat = self.mWr / (1 - beta1)\n",
    "        v_hat = self.vWr / (1 - beta2)\n",
    "        self.Wr -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wr)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mbr = beta1 * self.mbr + (1 - beta1) * self.dbr\n",
    "        self.vbr = beta2 * self.vbr + (1 - beta2) * np.square(self.dbr)\n",
    "        m_hat = self.mbr / (1 - beta1)\n",
    "        v_hat = self.vbr / (1 - beta2)\n",
    "        self.br -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.br)\n",
    "\n",
    "        # AdamW update for Wa\n",
    "        self.mWa = beta1 * self.mWa + (1 - beta1) * self.dWa\n",
    "        self.vWa = beta2 * self.vWa + (1 - beta2) * np.square(self.dWa)\n",
    "        m_hat = self.mWa / (1 - beta1)\n",
    "        v_hat = self.vWa / (1 - beta2)\n",
    "        self.Wa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wa)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for Wy\n",
    "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
    "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
    "        m_hat = self.mWy / (1 - beta1)\n",
    "        v_hat = self.vWy / (1 - beta2)\n",
    "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "        m_hat = self.mby / (1 - beta1)\n",
    "        v_hat = self.vby / (1 - beta2)\n",
    "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
    "\n",
    "    def train(self, data_generator,iterations):\n",
    "        \"\"\"\n",
    "        Train the GRU on a dataset using backpropagation through time.\n",
    "\n",
    "        Args:\n",
    "            data_generator: An instance of DataGenerator containing the training data.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        iter_num = 0\n",
    "        # stopping criterion for training\n",
    "        threshold = 50\n",
    "    \n",
    "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "        while (iter_num < iterations):\n",
    "            # initialize hidden state at the beginning of each sequence\n",
    "            if data_generator.pointer == 0:\n",
    "                c_prev = np.zeros((self.hidden_size, 1))\n",
    "                a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = data_generator.next_batch()\n",
    "\n",
    "            # forward pass\n",
    "            X, r, z, c, cc, a, y_pred = self.forward(inputs, c_prev, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[self.sequence_length - 1]\n",
    "            c_prev = c[self.sequence_length - 1]\n",
    "#             if iter_num == 5900 or iter_num == 30000:\n",
    "#                         self.learning_rate *= 0.1\n",
    "            # print progress every 100 iterations\n",
    "            if iter_num % 100 == 0:\n",
    "#                 self.learning_rate *= 0.99\n",
    "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
    "                print(''.join(data_generator.idx_to_char[idx] for idx in sample_idx))\n",
    "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
    "            iter_num += 1\n",
    "\n",
    "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
    "        \"\"\"\n",
    "        Sample a sequence of integers from the model.\n",
    "\n",
    "        Args:\n",
    "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
    "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
    "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
    "            n (int): Number of characters to generate.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize input and seed_idx\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        # convert one-hot encoding to integer index\n",
    "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
    "\n",
    "        # set the seed letter as the input for the first time step\n",
    "        x[seed_idx] = 1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        idxes = []\n",
    "        c = np.copy(c_prev)\n",
    "        a = np.copy(a_prev)\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "            c = z * c + (1 - z) * cc\n",
    "            a = c\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # append the sampled character to the sequence\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # return the generated sequence\n",
    "        return idxes\n",
    "\n",
    "    def predict(self, data_generator, start, n):\n",
    "        \"\"\"\n",
    "        Generate a sequence of n characters using the trained GRU model, starting from the given start sequence.\n",
    "\n",
    "        Args:\n",
    "        - data_generator: an instance of DataGenerator\n",
    "        - start: a string containing the start sequence\n",
    "        - n: an integer indicating the length of the generated sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "        # initialize input sequence\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        # initialize cell state and hidden state\n",
    "        a = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # generate new sequence of characters\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c = z * cc + (1 - z) * c\n",
    "\n",
    "            # compute the hidden state\n",
    "            a = c\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n",
    "        return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b3c17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T15:36:36.005659Z",
     "iopub.status.busy": "2023-03-13T15:36:36.004889Z",
     "iopub.status.idle": "2023-03-13T15:39:00.101802Z",
     "shell.execute_reply": "2023-03-13T15:39:00.099840Z"
    },
    "papermill": {
     "duration": 144.107821,
     "end_time": "2023-03-13T15:39:00.107725",
     "exception": false,
     "start_time": "2023-03-13T15:36:35.999904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TO3Ao&k&$xLJN:TMaL,N.CaWPec;nYDx\n",
      "wMnDV!,IZhEaFd$-GZZhAyz VkCysHKjDO3W\n",
      "mF HTP!JdkiJLswahLY3uhau\n",
      "hQzf?gI&hFObAo3i'dpl!k.KItZUE&gcdU,ZmjYIClEP&'V'wZKehjBd$iINDrXwHMttQGXFbnM UAeOLADsDp aL-JyFxlYm-MeKif p\n",
      "\n",
      "\n",
      "iter :0, loss:100.185232\n",
      "diNme h:vnnnn:zthoreCnen- erathhrFI\n",
      "iisoshne hos\n",
      "ictsierthne e eeeakr y al:iJrceieaf rthhTe arivswtse  eisitny:iltlnsnti e  ainpaG$'uarnydtincieyQn\n",
      "dacvluhhh&n gorukens th\n",
      "eta!fst iuoursa\n",
      " igwsou ous \n",
      "\n",
      "\n",
      "iter :100, loss:98.268607\n",
      "y\n",
      "soo wteen'e ath th iyt beo ute mmsrtitusreny aysuyth,  arp  sheaimto e m assnte toulrsthonp\n",
      "se awe tte ee ty mirtityilthhatm amuoud.\n",
      " meoor onu \n",
      "a?Sse at he  aahyzoud$\n",
      " bug,ue ssent  thears prganmyl\n",
      "\n",
      "\n",
      "iter :200, loss:95.675469\n",
      "t bbsut youed ryp w then d fier glllllyllod  bs,ea ysat bifthe be atrth,,e cindg-elrasrendle ymat boit it ftrite her e\n",
      "ngpre\n",
      "obmv aafed  bhalnil::\n",
      " sa\n",
      "\n",
      "Ind hat bb bfcoullrer yothe wiatre b ealk ea pec\n",
      "\n",
      "\n",
      "iter :300, loss:92.996750\n",
      "nd s, fblyllever btear,iznhin:\n",
      "\n",
      "F\n",
      "Fitelyle acndinvete wtheve alyou  bethe gartylhe ae wour mur emalizellyouer,A and pr:youlalyyo ouarramo fouis p,\n",
      "Fi\n",
      "vec.\n",
      " novu d iencselle the ayrivgat ange, ngndos d\n",
      "\n",
      "\n",
      "iter :400, loss:90.679335\n",
      "lhe thess the oyse ous haartins?yous tyound hese ad.\n",
      "ithog vti hecon gcousto hseper thoufign:g\n",
      "Ws\n",
      "\n",
      "M\n",
      "Wore tho famTacnd ot wyou ses than,\n",
      "WoumSth atncinth onou tha withe ousr mras po pour ho corbd atil\n",
      "\n",
      "\n",
      "iter :500, loss:88.705808\n",
      "iseeprmE\n",
      "Tud.\n",
      "he,\n",
      "e\n",
      "Frm me an\n",
      "TM\n",
      "Tn oon\n",
      "IWmS:\n",
      "Te' cpon:\n",
      "IIUthe Im iktins the, thee phsatil te ithehalnor mo ur too spend oru:\n",
      "pthalr parlo oron hu snhed\n",
      " cusi, cinomcorwurse the blis.\n",
      "Aoe'\n",
      "Th\n",
      "ai\n",
      "zn:\n",
      "M\n",
      "\n",
      "\n",
      "iter :600, loss:86.624839\n",
      "TMherems to me.\n",
      "\n",
      "MA:\n",
      "SENIUS:\n",
      "Ater\n",
      "Woe find deir tennd IUS:\n",
      "S:\n",
      "S:\n",
      "ENIUSS:\n",
      "Apound.\n",
      "\n",
      "A it pl\n",
      "TSNI?S-:\n",
      "T:ho 'de beelvy oundind tit, omi.\n",
      "Ou'sphe.\n",
      "I IUS:I mo\n",
      "Thnecdes\n",
      "TT:i;.\n",
      "TINIUS:\n",
      "SS:ein:ere yons e mfru.\n",
      "\n",
      "\n",
      "iter :700, loss:85.154694\n",
      "u mad likend h atrend mo uf yurd ha ayd IpalNe'eme,z adyo moud simk hien I beed wimeo.me\n",
      "OThat aekedid fooum.\n",
      "Tehen homud d I ffarckaullwy at orfik dea s wielld aurico ut ye ano\n",
      "TIUS\n",
      "Ase\n",
      "Te\n",
      "SAS:\n",
      "TUS:\n",
      "\n",
      "\n",
      "\n",
      "iter :800, loss:84.073567\n",
      " arfe hatomen tyi fo, agho beelS:\n",
      "anI\n",
      "SIUS:, theail inod srme s\n",
      "'haald seer d ebnbakel bceene morar tmank sg.\n",
      "F\n",
      "MENIIUS::\n",
      "h atmy ha fere neacuM!rad, mey he blle y whii vemar is digfho ase?.\n",
      "\n",
      "AAiof alT\n",
      "\n",
      "\n",
      "iter :900, loss:82.296793\n",
      "\n",
      "NII:I is ugom blelle:belis ith ys hous.\n",
      "A:\n",
      "S:\n",
      "OII?:\n",
      "IS::\n",
      "ARIUS:\n",
      "OIIIU:\n",
      "ARRo whoet wheat yus;, so Iwfhe\n",
      "harhe, tluloe, h yore vhandy or fofo bede sy lou wle:e rlonS: hencin tis higer tho,\n",
      "MuS:d ghit b\n",
      "\n",
      "\n",
      "iter :1000, loss:80.592793\n",
      " reso watist serlleelllals ivero,e\n",
      "Ofit hacle tus.\n",
      "\n",
      "MENI tha fuse, thsey, whi thisi stlo tther, tis sius;,Thik bseruts!o tr, ouf tre cislferad, tne nk\n",
      "MNIIUS:\n",
      "IIUS:\n",
      "Ohe st or fo foowre th able,\n",
      "AI:\n",
      "HR\n",
      "\n",
      "\n",
      "iter :1100, loss:78.966926\n",
      "n teareven twre tllilet fish is\n",
      "ARARII' gisallond, foricoubnse,\n",
      "\n",
      "F\n",
      "O\n",
      "Fi;s!\n",
      "MNIel anersd tha tiuds soder arthi streceres? bne ciwes?\n",
      "\n",
      "OTheent; pro, tyourcout b ronest! hiughe irmo t, hagadn, ay ithe ca\n",
      "\n",
      "\n",
      "iter :1200, loss:77.462634\n",
      "a enresM\n",
      "Hut!\n",
      "cenet le-rk, anor bun tyusak, an paver secarbe noewint ucacundI whag benod otar,d afo thean gant han no atyout nagan bdee thanet tous!;;-----------------k th papn ts ofe te npon foverigh\n",
      "\n",
      "\n",
      "iter :1300, loss:75.888593\n",
      "S:Hcho\n",
      "Ce!\n",
      "Agaprt, nere sha nche sw anet, nes Cth ambe why fefol-b\n",
      "\n",
      "Hs,\n",
      "Y-,\n",
      "Moelve wse theer fof mberweca vetey ugowren th signe t ouary tur ictha chens lad tews\n",
      "Be fares pof heuri,\n",
      "W:\n",
      "Fin hour nI ano\n",
      "\n",
      "\n",
      "iter :1400, loss:74.791623\n",
      "er se te meme;r\n",
      "HRLmIUS:\n",
      "OMNeCe fods ew, m.\n",
      "Whavhe iceres Citthue nge wilonds p\n",
      "Fit.\n",
      "\n",
      "Civeerey mo!usy, hy thear.\n",
      "\n",
      "Fit he holo mey Iod sothe and, m.\n",
      "HRIL\n",
      "HRICIUS: agerdey sowvee------------- amehe s po\n",
      "\n",
      "\n",
      "iter :1500, loss:73.665480\n",
      "a cikm Sondot ugs ndot lse layp ilod mate th yould fil.o\n",
      "\n",
      "LHRARARe tod oum,\n",
      "Whe y omo.\n",
      "Firt.\n",
      "BRCams cotet le vey cald igoud immy ous wis rotre y toh goour.\n",
      "\n",
      "Coto tho, omyo mey ove mod cofin coed cilth\n",
      "\n",
      "\n",
      "iter :1600, loss:72.734481\n",
      "t hearnod rwin glalld as beilll dad on,\n",
      "WLill,\n",
      "IRC oalill mout etwlout omet a lang tioubr tw'he y or ham imTer's lbelle douren beras ty buli lad bet st yad oor do wres\n",
      "B y omecadirege; th mes amy ilse\n",
      "\n",
      "\n",
      "iter :1700, loss:71.911291\n",
      ",\n",
      "YRIA\n",
      "Lnt file.\n",
      "\n",
      "VRVILkdae de\n",
      "ngooulum.\n",
      "\n",
      "Firseset arithary hy 'sondN all aad mit himy n faty ourd ibrin.\n",
      "\n",
      "VRIe willle pacirt, ga beleve\n",
      "ayo, to wtoure? acmend\n",
      "Thor ailefitey your fthee inot'e\n",
      "F\n",
      "LRACa\n",
      "\n",
      "\n",
      "iter :1800, loss:71.178896\n",
      "\n",
      "otous wlalso,\n",
      "LVingse t\n",
      "ou rtaces frinnorke shenestt\n",
      "Wheanf tha\n",
      "TLthean lladse arith\n",
      "Ls\n",
      "Vsf allve\n",
      "Vfithet,\n",
      "Wha te for wimes wint hor min thaserimes thetn chet ualsend iunciginristhes\n",
      "Vs,\n",
      "Tha\n",
      "AOMNIUS:\n",
      "\n",
      "\n",
      "iter :1900, loss:70.492503\n",
      "ruchangr thew wrch thef lfodi? ncke tensews\n",
      "Wha tangorot tinghe\n",
      "OLfI chas theeren.\n",
      "VOLftolin gthe.\n",
      "e\n",
      "COMIALnd;n cheetl.\n",
      "Lingis\n",
      "ARCIUS:\n",
      "\n",
      "Vwinst,\n",
      "\n",
      "VOILfir cat hiss chatisl\n",
      "VOLI' howhen wonh wolre whiess\n",
      "\n",
      "\n",
      "iter :2000, loss:69.688224\n",
      "efers anre the wouchas nthe anars't heathtte; preivhe wh atro rondef an terave men, nto.\n",
      "\n",
      "VOLLrkse tes pillait whe winte pan gergin ks,\n",
      "COnou s'ath out,\n",
      "Thou\n",
      "OVche sesper I'IU's\n",
      "Meors pthere:\n",
      "The\n",
      "VOLn\n",
      "\n",
      "\n",
      "iter :2100, loss:69.019170\n",
      "at\n",
      "it nthore wsound ndo to wtheng besse the weran anuch thore plers thalasl Rom.\n",
      "TUSENe of trot refe wasl onde thte horthethe nre, het heerd ingicim, antou Meche porkIA: arint, whh eroves anno.\n",
      "Y peng\n",
      "\n",
      "\n",
      "iter :2200, loss:68.660842\n",
      "tinar'do mto to\n",
      "st he partea the the toof ro ptoud nove tho ris\n",
      "LA: RIINUS:o\n",
      "TUSUa\n",
      "Thot,  efor nthe palt tomee nave pe rhaing he fe s kimet tofo mratnghe th moufs th alwe ponto ble:\n",
      "\n",
      "OLALtdon akime be\n",
      "\n",
      "\n",
      "iter :2300, loss:67.797608\n",
      "IA tout r ased her and the fracis hy fiuseiles fims foure vearve\n",
      "ALut oull\n",
      "COf:\n",
      "re:\n",
      "ACOpNo dpese tow\n",
      "OOLOA: aveaing set r' he momy be tain te, te te wire wore pplat th om peaveive, mo bant ackepal lda\n",
      "\n",
      "\n",
      "iter :2400, loss:67.423199\n",
      "bamebned de comand cot thee mint\n",
      "o bticomem iur met bedan-r, y,- ise\n",
      "pRGA: bes alty momo, tande tit gi: gir he morve' phet mey me'd agel ad\n",
      "eadiro dit hemdee teo and stheame?\n",
      "\n",
      "COMNIA: 'd iibuts tod ev\n",
      "\n",
      "\n",
      "iter :2500, loss:66.786262\n",
      " co\n",
      "nd\n",
      "COous wolli\n",
      "s\n",
      "Ficoun dot aple me,\n",
      "Wolangd de twireakt:\n",
      "e.\n",
      "\n",
      "Fis\n",
      "COINUS:\n",
      "Yoldse.\n",
      "COOf\n",
      "Oof icaviet wicol, s used, iveiicin akim tichem, mbal vee mebe uris\n",
      "\n",
      "Fios cannd\n",
      "BENINUUS:\n",
      "Yond med\n",
      "irsek lema\n",
      "\n",
      "\n",
      "iter :2600, loss:66.336793\n",
      "l scooulled temy,\n",
      ",\n",
      "Youl\n",
      "Fican, di coul wolust csiut Roube.\n",
      "COINS:\n",
      "CONIn tour mecer wees toour;s w owircon grend yu Romo,\n",
      "Y, ouse wios deit byu, wiste rcourd thisu, winvede.\n",
      "\n",
      "HENINUS: owird\n",
      "COOOLA:\n",
      "CO\n",
      "\n",
      "\n",
      "iter :2700, loss:65.965679\n",
      "eficensicto the my, woopuneconr, sound youce.\n",
      "Cma:\n",
      "HONIUS:orNUS:\n",
      "Yo!t\n",
      "chatered na'st hay oshiron dowrek shiury, s ilyo.\n",
      "\n",
      "COOMNeg:\n",
      "\n",
      "onwo notre\n",
      "and\n",
      "Fise so dend\n",
      "coe have my ur Rou bus yo herd-irt ate de\n",
      "\n",
      "\n",
      "iter :2800, loss:65.540901\n",
      "ony.\n",
      "\n",
      "BRIGNI:\n",
      "Ycoul walyy, se, Woung harot have bufre?\n",
      "hy,\n",
      "Yoret he'd yourenen theret, horist het hmiet\n",
      "ond?\n",
      "Wot, hate dis tay?\n",
      "Bip hony? busith; core\n",
      "b theman, yunshat,\n",
      "HIA y ous othate sreaveather i\n",
      "\n",
      "\n",
      "iter :2900, loss:64.746487\n",
      "d he fo nceann thas: hea'th. have ghan th hos whe rsu hath y'os sheom yore shane o\n",
      "pon!td hat yor's.\n",
      "Fot hat hthelilciss hy, yut unhe cotit.\n",
      "\n",
      "COININUS:I s, 'sat he thearveonceceirnt.\n",
      "\n",
      "Be hat haly, our\n",
      "\n",
      "\n",
      "iter :3000, loss:64.496109\n",
      "ot hiant\n",
      "hr: hat haghave now sh aveen'st orsibent ist\n",
      "CI RCairtt\n",
      "ond, se?\n",
      "Thanst wo lens:\n",
      "What? he pas benott han's at heres,\n",
      "Sor, sul satl thy omof hay son's shat lort heras ph herd,---b aty, alsat t\n",
      "\n",
      "\n",
      "iter :3100, loss:64.336126\n",
      "w anth senot ghave sel spelo bre thins\n",
      "Hranoty' he str hillby\n",
      "ulo. fo wat?\n",
      "he nos w harant?\n",
      "B.\n",
      "TUSINUS:\n",
      "SI I'll ear the spelll\n",
      "Bean's RI chanes?\n",
      "Th re thave anf- uto swd tirifnt.\n",
      "\n",
      "SINUS: ater an\n",
      "whary\n",
      "\n",
      "\n",
      "iter :3200, loss:64.059601\n",
      "eno y'Ture st, 'ope onn wles we wres bltize nkofer peulle btisel pne's fe ppope:t ud felil ficor.\n",
      "\n",
      "SI: bes toofr coullle the ypel obilc, wille fof fre th re wol bater foritzens.\n",
      "\n",
      "Fows fhe be\n",
      "HORGI ha \n",
      "\n",
      "\n",
      "iter :3300, loss:63.687369\n",
      "t bus unles\n",
      "Sou wont tro youbd oppeow reat the tro akenmos,\n",
      "Take wof he poll popcen's wofilebt at ifores,\n",
      "To' tient.\n",
      "Wa\n",
      "SICI'OLw\n",
      "Whe wo pno poull w wat poof lel to willbeg bere uthe get.\n",
      "Soont\n",
      "oci\n",
      "CII\n",
      "\n",
      "\n",
      "iter :3400, loss:63.233519\n",
      "e trere!\n",
      "S:\n",
      "Tho wre tices, foratrak' dour pake, pou bfgoe f conu!\n",
      "CIl'd dore mati\n",
      "pour RILAno wles pope ownoure pibll tlo' for pl\n",
      "Con\n",
      "ghir, wale gudse te oppetees mak ropille gamy.\n",
      "\n",
      "BRIUS:\n",
      "al'dse,\n",
      "\n",
      "TI\n",
      "\n",
      "\n",
      "iter :3500, loss:63.070779\n",
      "hoirke pore,\n",
      "Weced,--n- wath tre r\n",
      "Tmo peas\n",
      "'This waitn\n",
      "Thus thu tizece!\n",
      "waltdd at itch, tore dory, bagre the pecis, wifemim.\n",
      "\n",
      "MENIUS:I I' atigsie veoopte patime, ake panor,\n",
      "\n",
      "To,\n",
      "Th werast, thi ngarvi\n",
      "\n",
      "\n",
      "iter :3600, loss:62.799377\n",
      "oecmit wit nget bemie,\n",
      "Wise obllw, alde tatize mourd viicizne,\n",
      "We ppeis bel ay o'sur, Tmiass,\n",
      "Welo pou'd me oris,\n",
      "Thirs temousnd thice man;, w ord- mot peuget asufll, ti terak\n",
      "Thitrisr wieand inat tor\n",
      "\n",
      "\n",
      "iter :3700, loss:62.799479\n",
      "LAn t matitce sutd otth meo mtin.\n",
      "\n",
      "CORIOLAns, 'llt wa's anuld butteno ppages,\n",
      "Wechim.\n",
      "\n",
      "MENIUSI:\n",
      "P Thad thict hmier thes, hmins avei lids, sanursed me, dor atth arifrinaninis, And toudetraspeemu ad ter\n",
      "\n",
      "\n",
      "iter :3800, loss:62.748535\n",
      "minty th tizse, d det thins tipes,----\n",
      "'Th mase Mar ak tow dille sas mures?e\n",
      "th thra thle acandth cathecm, d Th aingith the ratrdemse: sres mal td ocues winds\n",
      "anns oth ati thcend\n",
      "Sends thtad sl'l gisi\n",
      "\n",
      "\n",
      "iter :3900, loss:62.423715\n",
      " imbe ther h mies:\n",
      "Fers, Youd rde maturd, atsibulelldes cmex thes peeccheso ml ads se manst he mses,------------\n",
      "\n",
      "PMENIIUS: wine manth ines lese nantsat do th eins th me somin frine!\n",
      "MENIUS:\n",
      "The sesll\n",
      "\n",
      "\n",
      "iter :4000, loss:62.336547\n",
      " met r heres\n",
      "wis, ad whe bet\n",
      "er,\n",
      "Yll angansh:\n",
      "Wee che lde geserauss.\n",
      "\n",
      "MENENIUS:\n",
      "HIOLANUSI:\n",
      "MENIIUS:\n",
      "CIIOLd seces the'lds asllde cesinesn then isn, mth ehealll becle tyo rinl\n",
      "Benes at\n",
      "ICIOLANUS:I Inl l\n",
      "\n",
      "\n",
      "iter :4100, loss:62.430603\n",
      " Se ardasy, o wheath milepe, peave heplle, Ypu wnduer qif othean pest oro chetthe beexath loce s Yoo my peas che rongth yo weresth bray, onghe vounses th wward cheex'I wn yor.\n",
      "\n",
      "MENENEIUS:\n",
      "IUS:\n",
      "MENIUS:\n",
      "\n",
      "\n",
      "iter :4200, loss:62.219287\n",
      ".\n",
      "ound tlo'd seces: thewe pl aw one veast hy othonuthy, o lolate vet of ormy.\n",
      "\n",
      "CIIOLANUS:\n",
      "IOLAd tof weat yo! whe avikene moenr.\n",
      "\n",
      "CORIOLAnagthe,  Yorom,\n",
      "\n",
      "BRUTUMout fod nof oth whor, wanld tho cante tho\n",
      "\n",
      "\n",
      "iter :4300, loss:62.429231\n",
      "n tyou fre atoreth;e bor'tt;e arenbe, baeast hay ow iro wole waty weart she por ibate gavie foret hogh tonc, bient byo ing haw hav athay belyo tolty g ofor atth byo head?\n",
      "Cothe thae thasy, awlo wattho\n",
      "\n",
      "\n",
      "iter :4400, loss:62.389617\n",
      "y't ay a wfou tryou ghat ay?\n",
      "ithy adou? bet bouley Hea to joul lfo\n",
      "whay whe: you, wthu to fo cartseq, was hoel\n",
      "y, on focote by ogange:\n",
      "\n",
      "Fof lacel, tyoug.\n",
      "\n",
      "COIOLANUS:\n",
      "yi thy our.\n",
      "\n",
      "MENIUS:\n",
      "\n",
      "BUS: Hay out\n",
      "\n",
      "\n",
      "iter :4500, loss:62.212258\n",
      "our. veingo, theas tlof byoug te svour pthatou; latd rveqourth tho shveatty oou.\n",
      "y thaty rougall t\n",
      "oru, thy oum tou doud nem\n",
      "Weivit. teims, w thtoer togur hay famtty our f has yoi pric. avifu fyour.\n",
      "C\n",
      "\n",
      "\n",
      "iter :4600, loss:62.347701\n",
      "ee't rou myour shay Thas speras se fle:\n",
      "o'd sht roit sand ism.\n",
      "\n",
      "CORIOLANUS:\n",
      "Whi spor te ibete' toun mou lloke men,\n",
      "i s;e.\n",
      "\n",
      "BRUThe,\n",
      "BUTUS:\n",
      "ANIUS:\n",
      "\n",
      "BRUTUS:\n",
      "ENIUS:\n",
      "Woret lele?\n",
      "y\n",
      "Thearvee?\n",
      "nt wands for et\n",
      "\n",
      "\n",
      "iter :4700, loss:62.309374\n",
      "sou sil'nit\n",
      "OLANUS:\n",
      "Dany. omud?y ice, s tandngo.\n",
      "Wheirv hivot rim\n",
      "thny, nicns mnar:\n",
      "\n",
      "CORIOLANUS:\n",
      "y oue? sken?\n",
      "Wheril, myau firm:\n",
      "Whey, Hy'ter soto chand pele?\n",
      "and;:\n",
      "Wher:\n",
      "\n",
      "Dand sy; you. us pervele se'\n",
      "\n",
      "\n",
      "iter :4800, loss:62.222287\n",
      "mim im mend sarve, idermen youm lal gast. herancent ind se'l wi ngasil m angim!\n",
      "Diste rincind\n",
      "Weigme?\n",
      "\n",
      "H'st th incous wlel tend nism ineds ngry.\n",
      "\n",
      "Fris y.\n",
      "He'lel mirevi nmem, ICNIUS:\n",
      "Wir wsearve riveim\n",
      "\n",
      "\n",
      "iter :4900, loss:62.113225\n",
      "ust sete; nde mand:\n",
      "Wear sit secend, beris singhirvi minen She tS I Sendiss ich insll wine nds itene.\n",
      "o\n",
      "chisri mend fir istele.\n",
      "Whond\n",
      "And meTen:\n",
      "Wher'ty i ghissit hand sachoum, sang me sm iingmaringd.\n",
      "\n",
      "\n",
      "iter :5000, loss:62.104469\n",
      "nd shawitnd imn chu?\n",
      "Theld mS:\n",
      "Heex'lld.\n",
      "\n",
      "SIOLUS:\n",
      "Thist hand beand ind hime in in gheve ecald sand temny ekiinsdond cim\n",
      "SI nichs thas far imint howls bard nest illy;\n",
      "onco mimy?\n",
      "\n",
      "Fris ther s ikend:\n",
      "Dir\n",
      "\n",
      "\n",
      "iter :5100, loss:62.189119\n",
      "eviond manad tthend Sot inggha wi s handd. oe\n",
      "SI ILUTidhstal!\n",
      "'ds panterd,\n",
      "Aunnda gme pcolken:\n",
      "Thal st tal dfoecthe chowind hism af hasokon\n",
      "Gnind I mS:\n",
      "ari\n",
      "dmes'd stis ivime smelondgle\n",
      "Whinds mant whi\n",
      "\n",
      "\n",
      "iter :5200, loss:62.265121\n",
      "ondn:\n",
      "ellow th thise chon tthoud tads anf ratw heas allf afis arvoer hth\n",
      "Thed Sured Seal; chal; mce\n",
      "Thisct hthe fates piinds, thead bue fThoolll theadndon hemelet atu sperit hise thirs ango at har has\n",
      "\n",
      "\n",
      "iter :5300, loss:62.251671\n",
      "he pure butamle fifr papotre\n",
      "Thil gme agd wotre thi ppiscicth, ancet whe wlofel wothist his hat bing ataked thi't Soe tho sate tou paanke, ft hats llok't hied the fot han agthu sh athou ft his ist ato\n",
      "\n",
      "\n",
      "iter :5400, loss:62.092162\n",
      "inas ther uf hicoud. hvei thotw ho et hout halst ot kee fto lote to for thad et bake wlood thick\n",
      "ICOLNUS:\n",
      "Gf he foor wald ute bu bu toure.\n",
      "\n",
      "Sepalet\n",
      "See thedro abtyei gou fouf arcusthe ff heeto frethut\n",
      "\n",
      "\n",
      "iter :5500, loss:62.091260\n",
      "The manord RTou tho thowrd;\n",
      "ots out hout hou fo ake pcoend wod, abue, foond; bule t oon had bu far's hat acous bot henod be, foord;\n",
      "ete fool the there pore, bulf oke'd Sacoe thecu hanim:\n",
      "Hil tous ther\n",
      "\n",
      "\n",
      "iter :5600, loss:62.213323\n",
      "ot tif goono beald onotl warois; pru, ad oug ufr anet'lle sole, us hoe. tohev erefo.\n",
      "\n",
      "CROROLUMNoontonethe tu foret hosutr, aganet buat oreppracoris,\n",
      "Anon:\n",
      "Hore toull aloe ongant hee worok,\n",
      "\n",
      "Fort his f\n",
      "\n",
      "\n",
      "iter :5700, loss:62.755780\n",
      "onengedreve erovee wothis los\n",
      "afr ou ond yonot hertirs\n",
      "gor mame torefrowes, toralll wory, has tor goon tor RIUThion hevein ges ofrUoo;b utefore mer ankoke ot'ddo lokre porens igm, wirse,\n",
      "Anlos wes fou\n",
      "\n",
      "\n",
      "iter :5800, loss:62.945836\n",
      "d gork ecat, natrte yu prathes racur batlo then wirs Ralllvo tasf loons-----\n",
      "Ofre.\n",
      "Yulll wen Roam!\n",
      "\n",
      "Frum RTha, and oonm,\n",
      "SI ILUTUS:\n",
      "GNENIUS:\n",
      "Yourgesu, fr usad con mar meres.\n",
      "\n",
      "Were,!\n",
      "Aus abut Roor?\n",
      "The\n",
      "\n",
      "\n",
      "iter :5900, loss:63.021547\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 24\n",
    "#read text from the \"input.txt\" file\n",
    "data_generator = DataGenerator('/kaggle/input/shakespeare-text/text.txt', sequence_length)\n",
    "gru =  GRU(hidden_size=100, vocab_size=data_generator.vocab_size,sequence_length=sequence_length,learning_rate=0.005)\n",
    "\n",
    "gru.train(data_generator,iterations=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbe6071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T15:39:00.151089Z",
     "iopub.status.busy": "2023-03-13T15:39:00.150082Z",
     "iopub.status.idle": "2023-03-13T15:39:00.545074Z",
     "shell.execute_reply": "2023-03-13T15:39:00.543198Z"
    },
    "papermill": {
     "duration": 0.423309,
     "end_time": "2023-03-13T15:39:00.551495",
     "exception": false,
     "start_time": "2023-03-13T15:39:00.128186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"c raveint hisr manse wane l on werwhir sy oucre thy ee gams thne tonds thocer yupppapcay, ne theant wis tons to lec kne wt's yar un my!\\n\\nBRUTore hins paredes\\nThold y oins!\\nOROLRUTont Senecan, Se\\nry,\\nYuprcuthouls,\\nAnes whes taspotraint k'o rtoreg\\ntat\\nisparcont so ntern-baless tarass not hrem, ngorut;\\nAt,\\nAnsu comrams cannt thinos aw iman thind veen ati blk, on wond voere d erviist one tille san th olo smane, wlor onu tht thes wers itin: tinb yaingerinvey.\\n\\nYoull?\\n\\nROLUMNIIUS:\\n\\nYornd mes\\nnat\\nAsso saess cass yu. treru\\npris upera, thut hor satr natru d\\nThorve y epreptregein gme meras rous thou pratisparublol yous cars-:\\nFr wsot wes yu pir on Rorhis,\\n\\nGIL wu panest w oon me thir mas'w thinm naunl\\nm Satin twirand whitnt inkpere\\n\\nThint Roncorms kik nore'sd shills tyorleveerd wale tonst ref ervee yu pear non mari thianes, lts as?\\ny uprat wined toll tudon tleve sexamt nat rint, an thiun thanlo urs.\\nHer wor co meer tins weriv gad aman gnor evir gome st onantak\\n, akit nucam cacnsit, ntines. Rathit\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.predict(data_generator, \"c\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960558a2",
   "metadata": {
    "papermill": {
     "duration": 0.019211,
     "end_time": "2023-03-13T15:39:00.593111",
     "exception": false,
     "start_time": "2023-03-13T15:39:00.573900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style='background:#FF9F00;border:0; color:black;\n",
    "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
    "    transform: rotateX(10deg);\n",
    "    '><center style='color: #3E3D53;'>Thank you</center></h1>\n",
    "\n",
    "# Thank you\n",
    "\n",
    "**Thank you for going through this notebook**\n",
    "\n",
    "**If you have any suggestions please let me know**\n",
    "\n",
    "<a id=\"5\"></a>\n",
    "# References \n",
    "https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "https://arxiv.org/abs/1711.05101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f273c",
   "metadata": {
    "papermill": {
     "duration": 0.020081,
     "end_time": "2023-03-13T15:39:00.635089",
     "exception": false,
     "start_time": "2023-03-13T15:39:00.615008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"padding:10px; \n",
    "            color:#333333;\n",
    "            margin:10px;\n",
    "            font-size:150%;\n",
    "            display:fill;\n",
    "            border-radius:1px;\n",
    "            border-style:solid;\n",
    "            border-color:#666666;\n",
    "            background-color:#F9F9F9;\n",
    "            overflow:hidden;\">\n",
    "    <center>\n",
    "        <a id='top'></a>\n",
    "        <b>Machine Learning From Scratch Series</b>\n",
    "    </center>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n",
    "        </li> \n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n",
    "        </li>\n",
    "        <li>\n",
    "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 156.776435,
   "end_time": "2023-03-13T15:39:01.376832",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-13T15:36:24.600397",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
